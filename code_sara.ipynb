{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "from torch.optim import RMSprop\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_time(t,T):\n",
    "    return 2*(t-T)/T+1\n",
    "\n",
    "def transformed_inventory_action(q0,q,x):\n",
    "    q_hat=q/q0-1\n",
    "    x_hat=x/q0\n",
    "    r=np.sqrt(q_hat**2+x_hat**2)\n",
    "    theta=np.arctan(-x_hat/q_hat)\n",
    "    chi=-x_hat/q_hat\n",
    "    if theta<=np.pi/4:\n",
    "        radial_dist=r*np.sqrt((chi**2+1)*(2*np.cos(np.pi/4-theta)**2))\n",
    "    else:\n",
    "        radial_dist=r*np.sqrt((chi**(-2)+1)*(2*np.cos(theta-np.pi/4)**2))\n",
    "    q_transform=-radial_dist*np.cos(theta)\n",
    "    x_transform=radial_dist*np.sin(theta)\n",
    "    return q_transform,x_transform\n",
    "\n",
    "def transformed_price(midprice_series):\n",
    "    \"\"\"\n",
    "    Computes the transformed price feature (P̃) from a time series of midprices.\n",
    "\n",
    "    Parameters:\n",
    "        midprice_series (pd.Series): Series indexed by timestamp (datetime), with midprice per second.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Transformed price series (P̃), same index as input.\n",
    "    \"\"\"\n",
    "    # Ensure the series is sorted by time\n",
    "    midprice_series = midprice_series.sort_index()\n",
    "\n",
    "    # Group by hour\n",
    "    grouped = midprice_series.groupby(midprice_series.index.floor('H'))\n",
    "\n",
    "    transformed_series = []\n",
    "\n",
    "    for hour, group in grouped:\n",
    "        # Subtract opening price of the hour\n",
    "        opening_price = group.iloc[0]\n",
    "        centered = group - opening_price\n",
    "\n",
    "        # Estimate scale to fit mostly within [-1, 1]\n",
    "        lower, upper = np.percentile(centered, [1, 99])  # clip only outliers\n",
    "        if lower!=upper:\n",
    "            # Affine transformation\n",
    "            transformed = (2/(upper-lower))*(centered-upper)+1\n",
    "        else:\n",
    "            transformed =0*centered\n",
    "        \n",
    "        transformed_series.append(transformed)\n",
    "\n",
    "    return pd.concat(transformed_series)\n",
    "\n",
    "def QV(midprice_series):\n",
    "    return np.sum(midprice_series.diff()**2)\n",
    "\n",
    "def transformed_qv(qv,qv_mean,qv_std):\n",
    "    return (qv-qv_mean)/(2*qv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv:\n",
    "    def __init__(self, start_date,T,N, delta_t,price_data, initial_inventory=500,a=1):\n",
    "        \"\"\"Time is expressed in second\"\"\"\n",
    "        self.start_date=start_date\n",
    "        self.T=T\n",
    "        self.delta_t=delta_t #period between succesive trades in min \n",
    "        self.Tk_list = np.array([T/N*i for i in range(1,N)])\n",
    "        self.Mk=T/N/delta_t\n",
    "        self.price_data = price_data\n",
    "        self.transormed_price=transformed_price(price_data)\n",
    "        self.initial_inventory = initial_inventory\n",
    "\n",
    "        self.current_period_index = 0\n",
    "        self.inventory = initial_inventory\n",
    "        self.time = self.Tk_list[0]\n",
    "        self.done = False\n",
    "        self.a=a\n",
    "\n",
    "        self.state=self.get_state(self.time)\n",
    "\n",
    "        _,self.qv_mean,self.qv_std=self.get_qv_mean_std()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_period_index = 0\n",
    "        self.inventory = self.initial_inventory\n",
    "        self.time = self.Tk_list[0]\n",
    "        self.done = False\n",
    "        return self.get_state(self.time)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Applique l'action (ex : quantité à vendre), met à jour l'état, retourne:\n",
    "        next_state, reward, done, info\n",
    "        \"\"\"\n",
    "        #Quantiti of shares to sell\n",
    "        x_Tk= action\n",
    "\n",
    "        #Select prices of the period following the action [T_k,T_k+1[\n",
    "        if self.current_period_index+1<len(self.Tk_list):\n",
    "            mask=(self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index])<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index+1]))\n",
    "        else:\n",
    "            mask=(self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index])<=self.price_data.index)\n",
    "\n",
    "        selected_times=self.price_data.index[mask]\n",
    "        prices=self.price_data.loc[selected_times]\n",
    "\n",
    "        #compute reward\n",
    "        reward =np.sum((self.inventory/prices.shape[0]) * prices.diff()-self.a*(x_Tk/self.Mk)**2)\n",
    "\n",
    "        self.inventory -= x_Tk\n",
    "\n",
    "        #Update current period\n",
    "        if self.current_period_index+1<len(self.Tk_list):\n",
    "            self.current_period_index += 1\n",
    "            #An episode ends when all the initial inventory has been sold \n",
    "            if self.inventory <= 0:\n",
    "                self.done = True\n",
    "        else:\n",
    "            self.done = True\n",
    "            \n",
    "        self.time = self.Tk_list[self.current_period_index]\n",
    "        next_state = self.get_state(self.time)\n",
    "\n",
    "\n",
    "        self.state=next_state\n",
    "        return next_state, reward, self.done, {}\n",
    "    \n",
    "    def get_state(self, T_i):\n",
    "\n",
    "        if self.current_period_index>0:\n",
    "            mask=(self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index-1])<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index]))\n",
    "        else:\n",
    "            mask=(self.start_date<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index]))\n",
    "            \n",
    "        selected_times=self.price_data.index[mask]\n",
    "        prices=self.price_data.loc[selected_times]\n",
    "        qv=QV(prices)\n",
    "\n",
    "        state = [\n",
    "            T_i,\n",
    "            self.inventory,\n",
    "            self.price_data.loc[self.start_date+timedelta(seconds=T_i):].values[0],\n",
    "            qv,\n",
    "        ]\n",
    "        return np.array(state, dtype=np.float32)\n",
    "    \n",
    "    def get_transformed_state(self,state,action):\n",
    "\n",
    "        time,q,price,qv=state\n",
    "\n",
    "        time_tr=transformed_time(time,self.T)\n",
    "        q_tr,_=transformed_inventory_action(self.initial_inventory,q,action)\n",
    "        price_tr=self.transormed_price.loc[self.start_date+timedelta(seconds=int(time)):].values[0]\n",
    "        qv_tr=transformed_qv(qv,self.qv_mean,self.qv_std)\n",
    "\n",
    "        \n",
    "        state = [\n",
    "            time_tr,\n",
    "            q_tr,\n",
    "            price_tr,\n",
    "            qv_tr,\n",
    "        ]\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def get_qv_mean_std(self):\n",
    "\n",
    "        list_qv=[]\n",
    "\n",
    "        for current_period_index in range(len(self.Tk_list)):\n",
    "            if current_period_index>0:\n",
    "                mask=(self.start_date+timedelta(seconds=self.Tk_list[current_period_index-1])<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[current_period_index]))\n",
    "            else:\n",
    "                mask=(self.start_date<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[current_period_index]))\n",
    "                \n",
    "            selected_times=self.price_data.index[mask]\n",
    "            prices=self.price_data.loc[selected_times]\n",
    "            qv=QV(prices)\n",
    "            list_qv.append(qv)\n",
    "\n",
    "        array_qv=np.array(list_qv)\n",
    "        return array_qv,np.mean(array_qv),np.std(array_qv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 1)  # Q-value output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class TradingAgentRL:\n",
    "    def __init__(self, env,state_dim, epsilon=0.1, tau=0.995, gamma=0.99, batch_size=5, memory_capacity=100, update_target_freq=10, lr=1e-3):\n",
    "        self.env=env\n",
    "        self.state_dim = state_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_freq = update_target_freq\n",
    "\n",
    "        self.memory = deque(maxlen=memory_capacity)\n",
    "        self.Q_main = QNetwork(state_dim)\n",
    "        self.Q_target = QNetwork(state_dim)\n",
    "        self.Q_target.load_state_dict(self.Q_main.state_dict())\n",
    "        self.optimizer = RMSprop(self.Q_main.parameters(), lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.iteration = 0\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        q_i = state[1] \n",
    "        T_i=state[0]\n",
    "\n",
    "        ## I we have reached terminal period [TN-1,T], we sell all the inventory \n",
    "        if T_i>=self.env.Tk_list[-1]:\n",
    "            action=q_i\n",
    "        else:\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                action = np.random.binomial(q_i,1/(self.env.T-T_i))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).repeat(int(q_i)+1,1)\n",
    "                    actions_tensor = torch.arange(0, int(q_i) + 1).float()\n",
    "\n",
    "                    ## Normalize inputs before using Q-network\n",
    "                    transformed_states_tensor = [\n",
    "                        torch.tensor(self.env.get_transformed_state(state, action), dtype=torch.float32)\n",
    "                        for state, action in zip(state_tensor, actions_tensor)\n",
    "                    ]\n",
    "                    transformed_states_tensor=torch.stack(transformed_states_tensor)\n",
    "                    inputs = torch.cat([transformed_states_tensor, actions_tensor.unsqueeze(1)], dim=1)\n",
    "                    #inputs = torch.cat([state_tensor, actions_tensor.unsqueeze(1)], dim=1)\n",
    "                    q_values = self.Q_main(inputs).squeeze()\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def train_step(self):\n",
    "\n",
    "        # We can update Q only if we have seen enough experience (state,action,reward,next_state)\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # We sample batch_size trasnitions from memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states = zip(*minibatch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "\n",
    "        # Target computation\n",
    "        with torch.no_grad():\n",
    "\n",
    "            next_q_values=[]\n",
    "            for j in range(self.batch_size):\n",
    "\n",
    "                q_range = int(next_states[j][1])  # inventory\n",
    "                ns_batch = next_states[j].unsqueeze(0).repeat(q_range + 1, 1)\n",
    "                actions_batch = torch.arange(0, q_range + 1).float()\n",
    "                \n",
    "                ### Normalize inputs before using Q-network\n",
    "                transformed_ns_batch = [\n",
    "                    torch.tensor(self.env.get_transformed_state(state, action), dtype=torch.float32)\n",
    "                    for state, action in zip(ns_batch, actions_batch.unsqueeze(1))\n",
    "                ]\n",
    "                transformed_ns_batch=torch.stack(transformed_ns_batch)\n",
    "                inputs = torch.cat([transformed_ns_batch, actions_batch.unsqueeze(1)], dim=1)\n",
    "                #inputs = torch.cat([ns_batch, actions_batch.unsqueeze(1)], dim=1)\n",
    "\n",
    "                if next_states[j][0]==self.env.T:\n",
    "                    next_q_value = 0\n",
    "                elif next_states[j][0]==self.env.Tk_list[-1]:\n",
    "                    #R(s,q)=q(p′ −p)−aq2,\n",
    "                    q=states[j][1]\n",
    "                    T=self.env.start_date+timedelta(seconds=self.env.T)\n",
    "                    delta=self.env.delta_t\n",
    "                    prices=self.env.price_data \n",
    "                    p=prices.loc[T:].values[0]\n",
    "                    p_prim=prices.loc[T+timedelta(seconds=delta):].values[0]\n",
    "                    next_q_value = q*(p_prim-p)-self.env.a*q**2\n",
    "                else:\n",
    "                    \n",
    "\n",
    "                    #Compute Q-values with main network\n",
    "                    self.Q_main.eval()\n",
    "                    ### Normalize inputs before using Q-network\n",
    "                    q_values = self.Q_main(inputs).squeeze()\n",
    "                    ns=torch.tensor(next_states[j])\n",
    "                    # Select action with highest Q-value\n",
    "                    action = torch.tensor([torch.argmax(q_values).item()])\n",
    "\n",
    "                    #Compute future Q-value with target network\n",
    "                    transformed_ns = torch.tensor(self.env.get_transformed_state(ns, action), dtype=torch.float32)\n",
    "                    input_target = torch.cat([transformed_ns, action]) \n",
    "                    #input_target = torch.cat([ns, action])  # shape: [5]\n",
    "                    self.Q_target.eval()\n",
    "                    next_q_value = self.Q_target(input_target.unsqueeze(0)).item()\n",
    "\n",
    "                next_q_values.append(next_q_value)        \n",
    "            \n",
    "            next_q_values = torch.FloatTensor(next_q_values)\n",
    "\n",
    "\n",
    "        targets=rewards+self.gamma*next_q_values\n",
    "\n",
    "        ### Normalize inputs before using Q-network\n",
    "        transformed_states = [\n",
    "            torch.tensor(self.env.get_transformed_state(state, action), dtype=torch.float32)\n",
    "            for state, action in zip(states, actions.unsqueeze(1))\n",
    "        ]\n",
    "        transformed_states=torch.stack(transformed_states)\n",
    "        inputs = torch.cat([transformed_states ,actions.unsqueeze(1)], dim=1)\n",
    "        q_preds = self.Q_main(inputs)\n",
    "        \n",
    "        loss = self.loss_fn(q_preds.squeeze(), targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "    def train(self, num_episodes=100, N=100, update_target_every=10, tau=0.995):\n",
    "        \"\"\"\n",
    "        Entraîne l'agent sur l'environnement `env` pendant `num_episodes` épisodes.\n",
    "        À chaque épisode, effectue N pas d'interaction.\n",
    "        \"\"\"\n",
    "        last_date=self.env.price_data.index[-1]\n",
    "        dates=self.env.price_data.loc[:last_date-timedelta(seconds=self.env.T)].index\n",
    "        for episode in range(num_episodes):\n",
    "            # One episode is defined as an execution period of lenght T, chosen randomly in the dataset (2018 to 2023)\n",
    "            self.env.start_date=random.choice(dates)\n",
    "            # State is reset at the beginning of each episode\n",
    "            state = self.env.reset()  \n",
    "            for i in range(N-1): # N is the number of period T0<T1..<TN-1 such that an action is taken at each T_i\n",
    "                \n",
    "                #choose action according to epsilon-greedy policy\n",
    "                action =self.choose_action(state)\n",
    "                # Update current state of the environment \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Save transition for experience replay\n",
    "                self.store_transition(state, action, reward, next_state)\n",
    "\n",
    "                state=next_state\n",
    "\n",
    "                # Update Q with experience replay\n",
    "                self.train_step()\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Mise à jour du réseau cible\n",
    "            if episode % update_target_every == 0:\n",
    "                self.Q_target.load_state_dict(self.Q_main.state_dict())\n",
    "\n",
    "            # Décroissance de ε\n",
    "            self.epsilon = max(self.epsilon * tau, 0.01)\n",
    "\n",
    "            print(f\"Episode {episode+1}/{num_episodes} terminé, ε = {self.epsilon:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Data/BTC_ETH_15mn.csv\")\n",
    "data.Date=data.Date.apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "data=data.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=TradingEnv(data.Price.index[0],15*500*60,100,15*60,data.Price)\n",
    "agent=TradingAgentRL(env,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cy/p_l7v2mn57v5jtdrn3qvp_0h0000gq/T/ipykernel_85070/3707553802.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ns=torch.tensor(next_states[j])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/2 terminé, ε = 0.0995\n",
      "Episode 2/2 terminé, ε = 0.0990\n"
     ]
    }
   ],
   "source": [
    "agent.train( num_episodes=2, N=70, update_target_every=10, tau=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnl_agent(price_series, actions, Mk, a):\n",
    "    \"\"\"\n",
    "    price_series : pd.Series index datetime, prix midprice à chaque tick\n",
    "    actions      : list of tuples (Tk_datetime, x_Tk) – quantité à vendre au début de chaque bloc\n",
    "    Mk           : nombre de pas de trading par bloc (env.Mk)\n",
    "    a            : pénalité quadratique (env.a)\n",
    "    \"\"\"\n",
    "    cash, penalty = 0.0, 0.0\n",
    "\n",
    "    for Tk_datetime, x in actions:\n",
    "        # 1) on récupère tous les prix entre Tk and Tk+1\n",
    "        next_time = Tk_datetime + timedelta(seconds=env.delta_t)\n",
    "        block = price_series.loc[Tk_datetime : next_time]\n",
    "        if len(block)==0:\n",
    "            continue\n",
    "\n",
    "        # 2) partage linéaire sur block\n",
    "        trades_per_step = x / len(block)\n",
    "        cash    += (trades_per_step * block).sum()\n",
    "        penalty += a * (trades_per_step ** 2) * len(block)  # ou *1 par step\n",
    "\n",
    "    return cash - penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnl_twap(price_series, Q0, Mk, N, a):\n",
    "    \"\"\"\n",
    "    TWAP: on vend Q0/N actions à chaque période Nk\n",
    "    \"\"\"\n",
    "    # date de début\n",
    "    start = price_series.index.min()\n",
    "    # reconstitue la liste des Tk\n",
    "    Tk_list = [start + timedelta(seconds=i * (env.T // N)) for i in range(N)]\n",
    "    actions = [(Tk, Q0 / N) for Tk in Tk_list]\n",
    "    return pnl_agent(price_series, actions, Mk, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, start_dates):\n",
    "    \"\"\"\n",
    "    start_dates : liste de datetimes à tester\n",
    "    Renvoie un dict {mean, median, glr, prob} des delta P&L vs TWAP en bp.\n",
    "    \"\"\"\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0   # mode pure greedy\n",
    "\n",
    "    deltas = []\n",
    "    for sd in start_dates:\n",
    "        # 1) repositionne l'env\n",
    "        agent.env.start_date = sd\n",
    "        state = agent.env.reset()\n",
    "\n",
    "        # 2) collect des (Tk_datetime, x_Tk)\n",
    "        actions = []\n",
    "        for Tk in agent.env.Tk_list:\n",
    "            action = agent.choose_action(state)\n",
    "            Tk_dt = sd + timedelta(seconds=int(Tk))\n",
    "            actions.append((Tk_dt, action))\n",
    "            state, _, done, _ = agent.env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # 3) calculs P&L\n",
    "        series = agent.env.price_data\n",
    "        pnl_rl  = pnl_agent(series, actions, agent.env.Mk, agent.env.a)\n",
    "        pnl_ref = pnl_twap(series, agent.env.initial_inventory,\n",
    "                          agent.env.Mk, len(agent.env.Tk_list),\n",
    "                          agent.env.a)\n",
    "\n",
    "        # 4) delta en basis points\n",
    "        deltas.append(1e4 * (pnl_rl - pnl_ref) / pnl_ref)\n",
    "\n",
    "    agent.epsilon = original_epsilon\n",
    "\n",
    "    arr = np.array(deltas)\n",
    "    stats = {\n",
    "        \"mean\"  : round(arr.mean(),3),\n",
    "        \"median\": round(np.median(arr),3),\n",
    "        \"glr\"   : round(arr[arr>0].mean() / (-arr[arr<0].mean() + 1e-8),3),\n",
    "        \"prob\"  : round(float((arr>0).mean()*100),3)\n",
    "    }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "min_time = data.index.min()\n",
    "max_time = data.index.max() - timedelta(seconds=env.T)\n",
    "# horaire plein toutes les T secondes\n",
    "test_dates = pd.date_range(min_time, max_time, freq=f\"{env.T}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = evaluate(agent, test_dates)\n",
    "print(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
