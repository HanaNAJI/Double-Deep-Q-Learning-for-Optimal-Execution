{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "from torch.optim import RMSprop\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Data/BTC_ETH_15mn.csv\")\n",
    "data.Date=data.Date.apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "data=data.set_index(\"Date\").fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_time(t,T):\n",
    "    return 2*(t-T)/T+1\n",
    "\n",
    "def transformed_inventory_action(q0,q,x):\n",
    "    q_hat=q/q0-1\n",
    "    x_hat=x/q0\n",
    "    r=np.sqrt(q_hat**2+x_hat**2)\n",
    "    theta=np.arctan(-x_hat/q_hat)\n",
    "    chi=-x_hat/q_hat\n",
    "    if theta<=np.pi/4:\n",
    "        radial_dist=r*np.sqrt((chi**2+1)*(2*np.cos(np.pi/4-theta)**2))\n",
    "    else:\n",
    "        radial_dist=r*np.sqrt((chi**(-2)+1)*(2*np.cos(theta-np.pi/4)**2))\n",
    "    q_transform=-radial_dist*np.cos(theta)\n",
    "    x_transform=radial_dist*np.sin(theta)\n",
    "    return q_transform,x_transform\n",
    "\n",
    "def transformed_price(midprice_series):\n",
    "    \"\"\"\n",
    "    Computes the transformed price feature (P̃) from a time series of midprices.\n",
    "\n",
    "    Parameters:\n",
    "        midprice_series (pd.Series): Series indexed by timestamp (datetime), with midprice per second.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Transformed price series (P̃), same index as input.\n",
    "    \"\"\"\n",
    "    # Ensure the series is sorted by time\n",
    "    midprice_series = midprice_series.sort_index()\n",
    "\n",
    "    # Group by hour\n",
    "    grouped = midprice_series.groupby(midprice_series.index.floor('H'))\n",
    "\n",
    "    transformed_series = []\n",
    "\n",
    "    for hour, group in grouped:\n",
    "        # Subtract opening price of the hour\n",
    "        opening_price = group.iloc[0]\n",
    "        centered = group - opening_price\n",
    "\n",
    "        # Estimate scale to fit mostly within [-1, 1]\n",
    "        lower, upper = np.percentile(centered, [1, 99])  # clip only outliers\n",
    "        if lower!=upper:\n",
    "            # Affine transformation\n",
    "            transformed = (2/(upper-lower))*(centered-upper)+1\n",
    "        else:\n",
    "            transformed =0*centered\n",
    "        \n",
    "        transformed_series.append(transformed)\n",
    "\n",
    "    return pd.concat(transformed_series)\n",
    "\n",
    "def QV(midprice_series):\n",
    "    return np.sum(midprice_series.diff()**2)\n",
    "\n",
    "def transformed_qv(qv,qv_mean,qv_std):\n",
    "    return (qv-qv_mean)/(2*qv_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv:\n",
    "    def __init__(self, start_date,T,N, delta_t,price_data, initial_inventory=500,a=1):\n",
    "        \"\"\"Time is expressed in second\"\"\"\n",
    "        self.start_date=start_date\n",
    "        self.T=T\n",
    "        self.delta_t=delta_t #period between succesive trades in min \n",
    "        self.Tk_list = np.array([T/N*i for i in range(1,N)])\n",
    "        self.Mk=T/N/delta_t\n",
    "        self.price_data = price_data\n",
    "        self.transormed_price=transformed_price(price_data)\n",
    "        self.initial_inventory = initial_inventory\n",
    "\n",
    "        self.current_period_index = 0\n",
    "        self.inventory = initial_inventory\n",
    "        self.time = self.Tk_list[0]\n",
    "        self.done = False\n",
    "        self.a=a\n",
    "\n",
    "        self.state=self.get_state(self.time)\n",
    "\n",
    "        _,self.qv_mean,self.qv_std=self.get_qv_mean_std()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_period_index = 0\n",
    "        self.inventory = self.initial_inventory\n",
    "        self.time = self.Tk_list[0]\n",
    "        self.done = False\n",
    "        return self.get_state(self.time)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Applique l'action (ex : quantité à vendre), met à jour l'état, retourne:\n",
    "        next_state, reward, done, info\n",
    "        \"\"\"\n",
    "        #Quantiti of shares to sell\n",
    "        x_Tk= action\n",
    "\n",
    "        #Select prices of the period following the action [T_k,T_k+1[\n",
    "        if self.current_period_index+1<len(self.Tk_list):\n",
    "\n",
    "            mask=(self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index])<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index+1]))\n",
    "        else:\n",
    "\n",
    "            mask=(self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index])<=self.price_data.index)\n",
    "\n",
    "        selected_times=self.price_data.index[mask]\n",
    "        prices=self.price_data.loc[selected_times]\n",
    "\n",
    "        inventory=self.inventory\n",
    "  \n",
    "        reward =np.sum((self.inventory/prices.shape[0]) * prices.diff()-self.a*(x_Tk/self.Mk)**2)\n",
    "\n",
    "\n",
    "        self.inventory -= x_Tk\n",
    "\n",
    "        #Update current period\n",
    "        if self.current_period_index+1<len(self.Tk_list):\n",
    "            self.current_period_index += 1\n",
    "            #An episode ends when all the initial inventory has been sold \n",
    "            if self.inventory <= 0:\n",
    "                self.done = True\n",
    "        else:\n",
    "            self.done = True\n",
    "            \n",
    "        self.time = self.Tk_list[self.current_period_index]\n",
    "        next_state = self.get_state(self.time)\n",
    "\n",
    "\n",
    "        self.state=next_state\n",
    "        return next_state, reward, self.done, {}\n",
    "    \n",
    "    def get_state(self, T_i):\n",
    "\n",
    "        if self.current_period_index>0:\n",
    "            mask=(self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index-1])<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index]))\n",
    "        else:\n",
    "            mask=(self.start_date<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index]))\n",
    "            \n",
    "        selected_times=self.price_data.index[mask]\n",
    "        prices=self.price_data.loc[selected_times]\n",
    "        qv=QV(prices)\n",
    "\n",
    "        state = [\n",
    "            T_i,\n",
    "            self.inventory,\n",
    "            self.price_data.loc[self.start_date+timedelta(seconds=T_i):].values[0],\n",
    "            qv,\n",
    "        ]\n",
    "        return np.array(state, dtype=np.float32)\n",
    "    \n",
    "    def get_transformed_state(self,state,action):\n",
    "\n",
    "        time,q,price,qv=state\n",
    "\n",
    "        time_tr=transformed_time(time,self.T)\n",
    "        q_tr,_=transformed_inventory_action(self.initial_inventory,q,action)\n",
    "        price_tr=self.transormed_price.loc[self.start_date+timedelta(seconds=int(time)):].values[0]\n",
    "        qv_tr=transformed_qv(qv,self.qv_mean,self.qv_std)\n",
    "\n",
    "        \n",
    "        state = [\n",
    "            time_tr,\n",
    "            q_tr,\n",
    "            price_tr,\n",
    "            qv_tr,\n",
    "        ]\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def get_qv_mean_std(self):\n",
    "\n",
    "        list_qv=[]\n",
    "\n",
    "        for current_period_index in range(len(self.Tk_list)):\n",
    "            if current_period_index>0:\n",
    "                mask=(self.start_date+timedelta(seconds=self.Tk_list[current_period_index-1])<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[current_period_index]))\n",
    "            else:\n",
    "                mask=(self.start_date<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[current_period_index]))\n",
    "                \n",
    "            selected_times=self.price_data.index[mask]\n",
    "            prices=self.price_data.loc[selected_times]\n",
    "            qv=QV(prices)\n",
    "            list_qv.append(qv)\n",
    "\n",
    "        array_qv=np.array(list_qv)\n",
    "        return array_qv,np.mean(array_qv),np.std(array_qv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 1)  # Q-value output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TradingAgentRL:\n",
    "    def __init__(self, env,state_dim, epsilon=0.1, tau=0.995, gamma=0.99, batch_size=5, memory_capacity=100, update_target_freq=10, lr=1e-3):\n",
    "        self.env=env\n",
    "        self.state_dim = state_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_freq = update_target_freq\n",
    "\n",
    "        self.memory = deque(maxlen=memory_capacity)\n",
    "        self.Q_main = QNetwork(state_dim)\n",
    "        self.Q_target = QNetwork(state_dim)\n",
    "        self.Q_target.load_state_dict(self.Q_main.state_dict())\n",
    "        self.optimizer = RMSprop(self.Q_main.parameters(), lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.iteration = 0\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        q_i = state[1] \n",
    "        T_i=state[0]\n",
    "\n",
    "        ## I we have reached terminal period [TN-1,T], we sell all the inventory \n",
    "        if T_i>=self.env.Tk_list[-1]:\n",
    "            action=q_i\n",
    "        else:\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                action = np.random.binomial(q_i,1/(self.env.T-T_i))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).repeat(int(q_i)+1,1)\n",
    "                    actions_tensor = torch.arange(0, int(q_i) + 1).float()\n",
    "\n",
    "                    ## Normalize inputs before using Q-network\n",
    "                    transformed_states_tensor = [\n",
    "                        torch.tensor(self.env.get_transformed_state(state, action), dtype=torch.float32)\n",
    "                        for state, action in zip(state_tensor, actions_tensor)\n",
    "                    ]\n",
    "                    transformed_states_tensor=torch.stack(transformed_states_tensor)\n",
    "                    inputs = torch.cat([transformed_states_tensor, actions_tensor.unsqueeze(1)], dim=1)\n",
    "                    #inputs = torch.cat([state_tensor, actions_tensor.unsqueeze(1)], dim=1)\n",
    "                    print(inputs)\n",
    "                    q_values = self.Q_main(inputs).squeeze()\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def train_step(self):\n",
    "\n",
    "        # We can update Q only if we have seen enough experience (state,action,reward,next_state)\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # We sample batch_size trasnitions from memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states = zip(*minibatch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "\n",
    "        # Target computation\n",
    "        with torch.no_grad():\n",
    "\n",
    "            next_q_values=[]\n",
    "            for j in range(self.batch_size):\n",
    "\n",
    "                q_range = int(next_states[j][1])  # inventory\n",
    "                ns_batch = next_states[j].unsqueeze(0).repeat(q_range + 1, 1)\n",
    "                actions_batch = torch.arange(0, q_range + 1).float()\n",
    "                \n",
    "                ### Normalize inputs before using Q-network\n",
    "                transformed_ns_batch = [\n",
    "                    torch.tensor(self.env.get_transformed_state(state, action), dtype=torch.float32)\n",
    "                    for state, action in zip(ns_batch, actions_batch.unsqueeze(1))\n",
    "                ]\n",
    "                transformed_ns_batch=torch.stack(transformed_ns_batch)\n",
    "                inputs = torch.cat([transformed_ns_batch, actions_batch.unsqueeze(1)], dim=1)\n",
    "                #inputs = torch.cat([ns_batch, actions_batch.unsqueeze(1)], dim=1)\n",
    "\n",
    "                if next_states[j][0]==self.env.T:\n",
    "                    next_q_value = 0\n",
    "                elif next_states[j][0]==self.env.Tk_list[-1]:\n",
    "                    #R(s,q)=q(p′ −p)−aq2,\n",
    "                    q=states[j][1]\n",
    "                    T=self.env.start_date+timedelta(seconds=self.env.T)\n",
    "                    delta=self.env.delta_t\n",
    "                    prices=self.env.price_data \n",
    "                    p=prices.loc[T:].values[0]\n",
    "                    p_prim=prices.loc[T+timedelta(seconds=delta):].values[0]\n",
    "                    next_q_value = q*(p_prim-p)-self.env.a*q**2\n",
    "                else:\n",
    "                    \n",
    "\n",
    "                    #Compute Q-values with main network\n",
    "                    self.Q_main.eval()\n",
    "                    ### Normalize inputs before using Q-network\n",
    "                    print(inputs)\n",
    "                    q_values = self.Q_main(inputs).squeeze()\n",
    "                    ns=torch.tensor(next_states[j])\n",
    "                    # Select action with highest Q-value\n",
    "                    action = torch.tensor([torch.argmax(q_values).item()])\n",
    "\n",
    "                    #Compute future Q-value with target network\n",
    "                    transformed_ns = torch.tensor(self.env.get_transformed_state(ns, action), dtype=torch.float32)\n",
    "                    input_target = torch.cat([transformed_ns, action]) \n",
    "                    #input_target = torch.cat([ns, action])  # shape: [5]\n",
    "                    self.Q_target.eval()\n",
    "                    next_q_value = self.Q_target(input_target.unsqueeze(0)).item()\n",
    "\n",
    "                next_q_values.append(next_q_value)        \n",
    "            \n",
    "            next_q_values = torch.FloatTensor(next_q_values)\n",
    "\n",
    "\n",
    "        targets=rewards+self.gamma*next_q_values\n",
    "\n",
    "        ### Normalize inputs before using Q-network\n",
    "        transformed_states = [\n",
    "            torch.tensor(self.env.get_transformed_state(state, action), dtype=torch.float32)\n",
    "            for state, action in zip(states, actions.unsqueeze(1))\n",
    "        ]\n",
    "        transformed_states=torch.stack(transformed_states)\n",
    "        inputs = torch.cat([transformed_states ,actions.unsqueeze(1)], dim=1)\n",
    "        q_preds = self.Q_main(inputs)\n",
    "        \n",
    "        loss = self.loss_fn(q_preds.squeeze(), targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "    def train(self, num_episodes=100, N=100, update_target_every=10, tau=0.995):\n",
    "        \"\"\"\n",
    "        Entraîne l'agent sur l'environnement `env` pendant `num_episodes` épisodes.\n",
    "        À chaque épisode, effectue N pas d'interaction.\n",
    "        \"\"\"\n",
    "        last_date=self.env.price_data.index[-1]\n",
    "        dates=self.env.price_data.loc[:last_date-timedelta(seconds=self.env.T)].index\n",
    "        for episode in range(num_episodes):\n",
    "            # One episode is defined as an execution period of lenght T, chosen randomly in the dataset (2018 to 2023)\n",
    "            self.env.start_date=random.choice(dates)\n",
    "            # State is reset at the beginning of each episode\n",
    "            state = self.env.reset()  \n",
    "            for i in range(N-1): # N is the number of period T0<T1..<TN-1 such that an action is taken at each T_i\n",
    "                \n",
    "                #choose action according to epsilon-greedy policy\n",
    "                action =self.choose_action(state)\n",
    "                # Update current state of the environment \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Save transition for experience replay\n",
    "                self.store_transition(state, action, reward, next_state)\n",
    "\n",
    "                state=next_state\n",
    "\n",
    "                # Update Q with experience replay\n",
    "                self.train_step()\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Mise à jour du réseau cible\n",
    "            if episode % update_target_every == 0:\n",
    "                self.Q_target.load_state_dict(self.Q_main.state_dict())\n",
    "\n",
    "            # Décroissance de ε\n",
    "            self.epsilon = max(self.epsilon * tau, 0.01)\n",
    "\n",
    "            print(f\"Episode {episode+1}/{num_episodes} terminé, ε = {self.epsilon:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnl_agent(price_series, actions, Mk, a):\n",
    "    \"\"\"\n",
    "    price_series : pd.Series index datetime, prix midprice à chaque tick\n",
    "    actions      : list of tuples (Tk_datetime, x_Tk) – quantité à vendre au début de chaque bloc\n",
    "    Mk           : nombre de pas de trading par bloc (env.Mk)\n",
    "    a            : pénalité quadratique (env.a)\n",
    "    \"\"\"\n",
    "    cash, penalty = 0.0, 0.0\n",
    "\n",
    "    for Tk_datetime, x in actions:\n",
    "        # 1) on récupère tous les prix entre Tk and Tk+1\n",
    "        next_time = Tk_datetime + timedelta(seconds=env.delta_t)\n",
    "        block = price_series.loc[Tk_datetime : next_time]\n",
    "        if len(block)==0:\n",
    "            continue\n",
    "\n",
    "        # 2) partage linéaire sur block\n",
    "        trades_per_step = x / len(block)\n",
    "        cash    += (trades_per_step * block).sum()\n",
    "        penalty += a * (trades_per_step ** 2) * len(block)  # ou *1 par step\n",
    "\n",
    "    return cash - penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnl_twap(price_series, Q0, Mk, N, a):\n",
    "    \"\"\"\n",
    "    TWAP: on vend Q0/N actions à chaque période Nk\n",
    "    \"\"\"\n",
    "    # date de début\n",
    "    start = price_series.index.min()\n",
    "    # reconstitue la liste des Tk\n",
    "    Tk_list = [start + timedelta(seconds=i * (env.T // N)) for i in range(N)]\n",
    "    actions = [(Tk, Q0 / N) for Tk in Tk_list]\n",
    "    return pnl_agent(price_series, actions, Mk, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, start_dates):\n",
    "    \"\"\"\n",
    "    start_dates : liste de datetimes à tester\n",
    "    Renvoie un dict {mean, median, glr, prob} des delta P&L vs TWAP en bp.\n",
    "    \"\"\"\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0   # mode pure greedy\n",
    "\n",
    "    deltas = []\n",
    "    for sd in start_dates:\n",
    "        # 1) repositionne l'env\n",
    "        agent.env.start_date = sd\n",
    "        state = agent.env.reset()\n",
    "\n",
    "        # 2) collect des (Tk_datetime, x_Tk)\n",
    "        actions = []\n",
    "        for Tk in agent.env.Tk_list:\n",
    "            action = agent.choose_action(state)\n",
    "            Tk_dt = sd + timedelta(seconds=int(Tk))\n",
    "            actions.append((Tk_dt, action))\n",
    "            state, _, done, _ = agent.env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # 3) calculs P&L\n",
    "        series = agent.env.price_data\n",
    "        pnl_rl  = pnl_agent(series, actions, agent.env.Mk, agent.env.a)\n",
    "        pnl_ref = pnl_twap(series, agent.env.initial_inventory,\n",
    "                          agent.env.Mk, len(agent.env.Tk_list),\n",
    "                          agent.env.a)\n",
    "\n",
    "        # 4) delta en basis points\n",
    "        deltas.append(1e4 * (pnl_rl - pnl_ref) / pnl_ref)\n",
    "\n",
    "    agent.epsilon = original_epsilon\n",
    "\n",
    "    arr = np.array(deltas)\n",
    "    stats = {\n",
    "        \"mean\"  : round(arr.mean(),3),\n",
    "        \"median\": round(np.median(arr),3),\n",
    "        \"glr\"   : round(arr[arr>0].mean() / (-arr[arr<0].mean() + 1e-8),3),\n",
    "        \"prob\"  : round(float((arr>0).mean()*100),3)\n",
    "    }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=TradingEnv(data.Price.index[0],15*500*60,100,15*60,data.Price)\n",
    "agent=TradingAgentRL(env,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cy/p_l7v2mn57v5jtdrn3qvp_0h0000gq/T/ipykernel_85070/3707553802.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ns=torch.tensor(next_states[j])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/2 terminé, ε = 0.0995\n",
      "Episode 2/2 terminé, ε = 0.0990\n"
     ]
    }
   ],
   "source": [
    "agent.train( num_episodes=2, N=70, update_target_every=10, tau=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cy/p_l7v2mn57v5jtdrn3qvp_0h0000gq/T/ipykernel_89300/3599390174.py:8: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  theta=np.arctan(-x_hat/q_hat)\n",
      "/var/folders/cy/p_l7v2mn57v5jtdrn3qvp_0h0000gq/T/ipykernel_89300/3599390174.py:9: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  chi=-x_hat/q_hat\n",
      "/var/folders/cy/p_l7v2mn57v5jtdrn3qvp_0h0000gq/T/ipykernel_89300/3599390174.py:8: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  theta=np.arctan(-x_hat/q_hat)\n",
      "/var/folders/cy/p_l7v2mn57v5jtdrn3qvp_0h0000gq/T/ipykernel_89300/3599390174.py:9: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  chi=-x_hat/q_hat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.8000e-01,         nan,  7.7559e-01,  2.3863e-01,  0.0000e+00],\n",
      "        [-9.8000e-01,        -inf,  7.7559e-01,  2.3863e-01,  1.0000e+00],\n",
      "        [-9.8000e-01,        -inf,  7.7559e-01,  2.3863e-01,  2.0000e+00],\n",
      "        ...,\n",
      "        [-9.8000e-01,        -inf,  7.7559e-01,  2.3863e-01,  4.9800e+02],\n",
      "        [-9.8000e-01,        -inf,  7.7559e-01,  2.3863e-01,  4.9900e+02],\n",
      "        [-9.8000e-01,        -inf,  7.7559e-01,  2.3863e-01,  5.0000e+02]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (501x5 and 6x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     11\u001b[0m agent \u001b[38;5;241m=\u001b[39m TradingAgentRL(\n\u001b[1;32m     12\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[1;32m     13\u001b[0m     state_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,                    \u001b[38;5;66;03m# Dimension de l'état transformé\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     use_binomial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m               \u001b[38;5;66;03m# Utilise la loi binomiale pour l'exploration\u001b[39;00m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 3. Entraînement avec vos paramètres\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m     26\u001b[0m     num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,                 \u001b[38;5;66;03m# Nombre d'épisodes d'entraînement\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     steps_per_episode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m,           \u001b[38;5;66;03m# Nombre de pas par épisode (N=70)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     update_target_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m          \u001b[38;5;66;03m# Mise à jour du réseau cible\u001b[39;00m\n\u001b[1;32m     29\u001b[0m )\n",
      "Cell \u001b[0;32mIn[14], line 306\u001b[0m, in \u001b[0;36mTradingAgentRL.train\u001b[0;34m(self, num_episodes, steps_per_episode, update_target_every)\u001b[0m\n\u001b[1;32m    303\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps_per_episode):\n\u001b[0;32m--> 306\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[1;32m    307\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state)\n",
      "Cell \u001b[0;32mIn[14], line 215\u001b[0m, in \u001b[0;36mTradingAgentRL.choose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Calcul des Q-values\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs)\n\u001b[0;32m--> 215\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_main(inputs)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmax(q_values)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 148\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    146\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 148\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)))\n\u001b[1;32m    149\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m    150\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (501x5 and 6x20)"
     ]
    }
   ],
   "source": [
    "# 1. Initialisation de l'environnement avec vos paramètres\n",
    "env = TradingEnv(\n",
    "    start_date=data.Price.index[0],  # Date de début des données\n",
    "    T=15*500*60,                    # 500 périodes de 15 minutes (en secondes)\n",
    "    N=100,                          # 100 pas de trading\n",
    "    delta_t=15*60,                  # 15 minutes entre les trades (en secondes)\n",
    "    price_data=data.Price           # Série des prix\n",
    ")\n",
    "\n",
    "# 2. Initialisation de l'agent avec état de dimension 5\n",
    "agent = TradingAgentRL(\n",
    "    env=env,\n",
    "    state_dim=5,                    # Dimension de l'état transformé\n",
    "    epsilon=0.1,                    # Taux d'exploration initial\n",
    "    tau=0.995,                      # Facteur de décroissance d'epsilon\n",
    "    gamma=0.99,                     # Facteur d'actualisation\n",
    "    batch_size=64,                  # Taille du batch (augmentée pour stabilité)\n",
    "    memory_capacity=10000,          # Capacité de la mémoire (augmentée)\n",
    "    update_target_freq=10,          # Fréquence de mise à jour du réseau cible\n",
    "    lr=1e-4,                        # Taux d'apprentissage (réduit pour stabilité)\n",
    "    use_binomial=True               # Utilise la loi binomiale pour l'exploration\n",
    ")\n",
    "\n",
    "# 3. Entraînement avec vos paramètres\n",
    "agent.train(\n",
    "    num_episodes=2,                 # Nombre d'épisodes d'entraînement\n",
    "    steps_per_episode=70,           # Nombre de pas par épisode (N=70)\n",
    "    update_target_every=10          # Mise à jour du réseau cible\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "min_time = data.index.min()\n",
    "max_time = data.index.max() - timedelta(seconds=env.T)\n",
    "# horaire plein toutes les T secondes\n",
    "test_dates = pd.date_range(min_time, max_time, freq=f\"{env.T}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = evaluate(agent, test_dates)\n",
    "print(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
