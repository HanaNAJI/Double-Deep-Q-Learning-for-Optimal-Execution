{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yasser/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "from torch.optim import RMSprop\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_time(t,T):\n",
    "    return 2*(t-T)/T+1\n",
    "\n",
    "def transformed_inventory_action(q0,q,x):\n",
    "    q_hat=q/q0-1\n",
    "    x_hat=x/q0\n",
    "    r=np.sqrt(q_hat**2+x_hat**2)\n",
    "    theta=np.arctan(-x_hat/q_hat)\n",
    "    chi=-x_hat/q_hat\n",
    "    if theta<=np.pi/4:\n",
    "        radial_dist=r*np.sqrt((chi**2+1)*(2*np.cos(np.pi/4-theta)**2))\n",
    "    else:\n",
    "        radial_dist=r*np.sqrt((chi**(-2)+1)*(2*np.cos(theta-np.pi/4)**2))\n",
    "    q_transform=-radial_dist*np.cos(theta)\n",
    "    x_transform=radial_dist*np.sin(theta)\n",
    "    return q_transform,x_transform\n",
    "\n",
    "def transformed_price(midprice_series):\n",
    "    \"\"\"\n",
    "    Computes the transformed price feature (P̃) from a time series of midprices.\n",
    "\n",
    "    Parameters:\n",
    "        midprice_series (pd.Series): Series indexed by timestamp (datetime), with midprice per second.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Transformed price series (P̃), same index as input.\n",
    "    \"\"\"\n",
    "    # Ensure the series is sorted by time\n",
    "    midprice_series = midprice_series.sort_index()\n",
    "\n",
    "    # Group by hour\n",
    "    grouped = midprice_series.groupby(midprice_series.index.floor('H'))\n",
    "\n",
    "    transformed_series = []\n",
    "\n",
    "    for hour, group in grouped:\n",
    "        # Subtract opening price of the hour\n",
    "        opening_price = group.iloc[0]\n",
    "        centered = group - opening_price\n",
    "\n",
    "        # Estimate scale to fit mostly within [-1, 1]\n",
    "        lower, upper = np.percentile(centered, [1, 99])  # clip only outliers\n",
    "        if lower!=upper:\n",
    "            # Affine transformation\n",
    "            transformed = (2/(upper-lower))*(centered-upper)+1\n",
    "        else:\n",
    "            transformed =0*centered\n",
    "        \n",
    "        transformed_series.append(transformed)\n",
    "\n",
    "    return pd.concat(transformed_series)\n",
    "\n",
    "def QV(midprice_series):\n",
    "    return np.sum(midprice_series.diff()**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv:\n",
    "    def __init__(self, start_date,T,N, delta_t,price_data, initial_inventory=500,a=1):\n",
    "        \"\"\"Time is expressed in second\"\"\"\n",
    "        self.start_date=start_date\n",
    "        self.T=T\n",
    "        self.delta_t=delta_t #period between succesive trades in min \n",
    "        self.Tk_list = np.array([T/N*i for i in range(1,N)])\n",
    "        self.Mk=T/N/delta_t\n",
    "        self.price_data = price_data\n",
    "        self.transormed_price=transformed_price(price_data)\n",
    "        self.initial_inventory = initial_inventory\n",
    "\n",
    "        self.current_period_index = 0\n",
    "        self.inventory = initial_inventory\n",
    "        self.time = self.Tk_list[0]\n",
    "        self.done = False\n",
    "        self.a=a\n",
    "\n",
    "        self.state=self.get_state(self.time)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_period_index = 0\n",
    "        self.inventory = self.initial_inventory\n",
    "        self.time = self.Tk_list[0]\n",
    "        self.done = False\n",
    "        return self.get_state(self.time)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Applique l'action (ex : quantité à vendre), met à jour l'état, retourne:\n",
    "        next_state, reward, done, info\n",
    "        \"\"\"\n",
    "        #Quantiti of shares to sell\n",
    "        x_Tk= action\n",
    "\n",
    "        #Select prices of the period following the action [T_k,T_k+1[\n",
    "        mask=(self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index])<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index+1]))\n",
    "        selected_times=self.price_data.index[mask]\n",
    "        prices=self.price_data.loc[selected_times]\n",
    "\n",
    "        #compute reward\n",
    "        reward =np.sum((self.inventory/prices.shape[0]) * prices.diff()-self.a*(x_Tk/self.Mk)**2)\n",
    "\n",
    "        self.inventory -= x_Tk\n",
    "\n",
    "        #Update current period\n",
    "        self.current_period_index += 1\n",
    "\n",
    "        #An episode ends when all the initial inventory has been sold \n",
    "        if self.inventory <= 0:\n",
    "            self.done = True\n",
    "\n",
    "        self.time = self.Tk_list[self.current_period_index]\n",
    "        next_state = self.get_state(self.time)\n",
    "\n",
    "\n",
    "        self.state=next_state\n",
    "        return next_state, reward, self.done, {}\n",
    "    \n",
    "    def get_state(self, T_i):\n",
    "\n",
    "        if self.current_period_index>0:\n",
    "            mask=(self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index-1])<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index]))\n",
    "        else:\n",
    "            mask=(self.start_date<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index]))\n",
    "            \n",
    "        selected_times=self.price_data.index[mask]\n",
    "        prices=self.price_data.loc[selected_times]\n",
    "        qv=QV(prices)\n",
    "\n",
    "        state = [\n",
    "            T_i,\n",
    "            self.inventory,\n",
    "            self.price_data.loc[self.start_date+timedelta(seconds=T_i):].values[0],\n",
    "            qv,\n",
    "        ]\n",
    "        return np.array(state, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 1)  # Q-value output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class TradingAgentRL:\n",
    "    def __init__(self, env,state_dim, epsilon=0.1, tau=0.995, gamma=0.99, batch_size=5, memory_capacity=100, update_target_freq=10, lr=1e-3):\n",
    "        self.env=env\n",
    "        self.state_dim = state_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_freq = update_target_freq\n",
    "\n",
    "        self.memory = deque(maxlen=memory_capacity)\n",
    "        self.Q_main = QNetwork(state_dim)\n",
    "        self.Q_target = QNetwork(state_dim)\n",
    "        self.Q_target.load_state_dict(self.Q_main.state_dict())\n",
    "        self.optimizer = RMSprop(self.Q_main.parameters(), lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.iteration = 0\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        q_i = state[1] \n",
    "        T_i=state[0]\n",
    "\n",
    "        ## I we have reached terminal period [TN-1,T], we sell all the inventory \n",
    "        if T_i>=self.env.Tk_list[-1]:\n",
    "            action=q_i\n",
    "        else:\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                action = np.random.binomial(q_i,1/(self.env.T-T_i))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).repeat(int(q_i)+1,1)\n",
    "                    actions_tensor = torch.arange(0, int(q_i) + 1).float()\n",
    "                    inputs = torch.cat([state_tensor, actions_tensor.unsqueeze(1)], dim=1)\n",
    "                    q_values = self.Q_main(inputs).squeeze()\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def train_step(self):\n",
    "\n",
    "        # We can update Q only if we have seen enough experience (state,action,reward,next_state)\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # We sample batch_size trasnitions from memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states = zip(*minibatch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "\n",
    "        # Target computation\n",
    "        with torch.no_grad():\n",
    "\n",
    "            next_q_values=[]\n",
    "            for j in range(self.batch_size):\n",
    "\n",
    "                q_range = int(next_states[j][1])  # inventory\n",
    "                ns_batch = next_states[j].unsqueeze(0).repeat(q_range + 1, 1)\n",
    "                actions_batch = torch.arange(0, q_range + 1).float()\n",
    "                inputs = torch.cat([ns_batch, actions_batch.unsqueeze(1)], dim=1)\n",
    "\n",
    "                if next_states[j][0]==self.env.T:\n",
    "                    next_q_value = 0\n",
    "                elif next_states[j][0]==self.env.Tk_list[-1]:\n",
    "                    #R(s,q)=q(p′ −p)−aq2,\n",
    "                    q=states[j][1]\n",
    "                    T=self.env.start_date+timedelta(seconds=self.env.T)\n",
    "                    delta=self.env.delta_t\n",
    "                    prices=self.env.price_data \n",
    "                    p=prices.loc[T:].values[0]\n",
    "                    p_prim=prices.loc[T+timedelta(seconds=delta):].values[0]\n",
    "                    next_q_value = q*(p_prim-p)-self.env.a*q**2#f(next_states[j],states[j][1])\n",
    "                else:\n",
    "                    \n",
    "\n",
    "                    #Compute Q-values with main network\n",
    "                    self.Q_main.eval()\n",
    "                    q_values = self.Q_main(inputs).squeeze()\n",
    "                    ns=torch.tensor(next_states[j])\n",
    "                    # Select action with highest Q-value\n",
    "                    action = torch.tensor([torch.argmax(q_values).item()])\n",
    "                    #Compute future Q-value with target network\n",
    "                    input_target = torch.cat([ns, action])  # shape: [5]\n",
    "                    self.Q_target.eval()\n",
    "                    next_q_value = self.Q_target(input_target.unsqueeze(0)).item()\n",
    "\n",
    "                next_q_values.append(next_q_value)        \n",
    "            \n",
    "            next_q_values = torch.FloatTensor(next_q_values)\n",
    "\n",
    "\n",
    "        targets=rewards+self.gamma*next_q_values\n",
    "        inputs = torch.cat([states ,actions.unsqueeze(1)], dim=1)\n",
    "        q_preds = self.Q_main(inputs)\n",
    "        loss = self.loss_fn(q_preds.squeeze(), targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "    def train(self, num_episodes=100, N=100, update_target_every=10, tau=0.995):\n",
    "        \"\"\"\n",
    "        Entraîne l'agent sur l'environnement `env` pendant `num_episodes` épisodes.\n",
    "        À chaque épisode, effectue N pas d'interaction.\n",
    "        \"\"\"\n",
    "        last_date=self.env.price_data.index[-1]\n",
    "        dates=self.env.price_data.loc[:last_date-timedelta(seconds=self.env.T)].index\n",
    "        for episode in range(num_episodes):\n",
    "            # One episode is defined as an execution period of lenght T, chosen randomly in the dataset (2018 to 2023)\n",
    "            self.env.start_date=random.choice(dates)\n",
    "            # State is reset at the beginning of each episode\n",
    "            state = self.env.reset()  \n",
    "            for i in range(N): # N is the number of period T0<T1..<TN-1 such that an action is taken at each T_i\n",
    "\n",
    "                #choose action according to epsilon-greedy policy\n",
    "                action =self.choose_action(state)\n",
    "\n",
    "                # Update current state of the environment \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Save transition for experience replay\n",
    "                self.store_transition(state, action, reward, next_state)\n",
    "\n",
    "                state=next_state\n",
    "\n",
    "                # Update Q with experience replay\n",
    "                self.train_step()\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Mise à jour du réseau cible\n",
    "            if episode % update_target_every == 0:\n",
    "                self.Q_target.load_state_dict(self.Q_main.state_dict())\n",
    "\n",
    "            # Décroissance de ε\n",
    "            self.epsilon = max(self.epsilon * tau, 0.01)\n",
    "\n",
    "            print(f\"Episode {episode+1}/{num_episodes} terminé, ε = {self.epsilon:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Data/BTC_ETH_15mn.csv\")\n",
    "data.Date=data.Date.apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "data=data.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Price (ETH)</th>\n",
       "      <th>Volume (ETH)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-10-24 00:00:00</th>\n",
       "      <td>6476.540000</td>\n",
       "      <td>3.708780e+09</td>\n",
       "      <td>204.327000</td>\n",
       "      <td>1232950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24 00:15:00</th>\n",
       "      <td>6483.150000</td>\n",
       "      <td>3.709670e+09</td>\n",
       "      <td>204.378000</td>\n",
       "      <td>1236570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24 00:30:00</th>\n",
       "      <td>6474.200000</td>\n",
       "      <td>3.677130e+09</td>\n",
       "      <td>204.439000</td>\n",
       "      <td>1239840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24 00:45:00</th>\n",
       "      <td>6486.190000</td>\n",
       "      <td>3.661460e+09</td>\n",
       "      <td>204.441000</td>\n",
       "      <td>1237220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24 01:00:00</th>\n",
       "      <td>6480.084479</td>\n",
       "      <td>3.641914e+09</td>\n",
       "      <td>204.070635</td>\n",
       "      <td>1227899090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-27 08:45:00</th>\n",
       "      <td>37346.223298</td>\n",
       "      <td>1.628826e+10</td>\n",
       "      <td>2046.790071</td>\n",
       "      <td>9322101316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-27 09:00:00</th>\n",
       "      <td>37401.430405</td>\n",
       "      <td>1.635176e+10</td>\n",
       "      <td>2049.064393</td>\n",
       "      <td>9328586333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-27 09:15:00</th>\n",
       "      <td>37405.996546</td>\n",
       "      <td>1.648487e+10</td>\n",
       "      <td>2049.523629</td>\n",
       "      <td>9353842431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-27 09:30:00</th>\n",
       "      <td>37432.627567</td>\n",
       "      <td>1.656480e+10</td>\n",
       "      <td>2051.479080</td>\n",
       "      <td>9362712712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-27 09:45:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9366213874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178444 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Price        Volume  Price (ETH)  Volume (ETH)\n",
       "Date                                                                      \n",
       "2018-10-24 00:00:00   6476.540000  3.708780e+09   204.327000    1232950000\n",
       "2018-10-24 00:15:00   6483.150000  3.709670e+09   204.378000    1236570000\n",
       "2018-10-24 00:30:00   6474.200000  3.677130e+09   204.439000    1239840000\n",
       "2018-10-24 00:45:00   6486.190000  3.661460e+09   204.441000    1237220000\n",
       "2018-10-24 01:00:00   6480.084479  3.641914e+09   204.070635    1227899090\n",
       "...                           ...           ...          ...           ...\n",
       "2023-11-27 08:45:00  37346.223298  1.628826e+10  2046.790071    9322101316\n",
       "2023-11-27 09:00:00  37401.430405  1.635176e+10  2049.064393    9328586333\n",
       "2023-11-27 09:15:00  37405.996546  1.648487e+10  2049.523629    9353842431\n",
       "2023-11-27 09:30:00  37432.627567  1.656480e+10  2051.479080    9362712712\n",
       "2023-11-27 09:45:00           NaN           NaN          NaN    9366213874\n",
       "\n",
       "[178444 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=TradingEnv(data.Price.index[0],15*500*60,100,15*60,data.Price)\n",
    "agent=TradingAgentRL(env,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-e62f627eef0b>:85: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  states = torch.FloatTensor(states)\n",
      "<ipython-input-4-e62f627eef0b>:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ns=torch.tensor(next_states[j])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/2 terminé, ε = 0.0995\n",
      "Episode 2/2 terminé, ε = 0.0990\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent.train( num_episodes=2, N=70, update_target_every=10, tau=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnl_agent(price_series, actions, Mk, a):\n",
    "    \"\"\"\n",
    "    price_series : pd.Series index datetime, prix midprice à chaque tick\n",
    "    actions      : list of tuples (Tk_datetime, x_Tk) – quantité à vendre au début de chaque bloc\n",
    "    Mk           : nombre de pas de trading par bloc (env.Mk)\n",
    "    a            : pénalité quadratique (env.a)\n",
    "    \"\"\"\n",
    "    cash, penalty = 0.0, 0.0\n",
    "\n",
    "    for Tk_datetime, x in actions:\n",
    "        # 1) on récupère tous les prix entre Tk and Tk+1\n",
    "        next_time = Tk_datetime + timedelta(seconds=env.delta_t)\n",
    "        block = price_series.loc[Tk_datetime : next_time]\n",
    "        if len(block)==0:\n",
    "            continue\n",
    "\n",
    "        # 2) partage linéaire sur block\n",
    "        trades_per_step = x / len(block)\n",
    "        cash    += (trades_per_step * block).sum()\n",
    "        penalty += a * (trades_per_step ** 2) * len(block)  # ou *1 par step\n",
    "\n",
    "    return cash - penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnl_twap(price_series, Q0, Mk, N, a):\n",
    "    \"\"\"\n",
    "    TWAP: on vend Q0/N actions à chaque période Nk\n",
    "    \"\"\"\n",
    "    # date de début\n",
    "    start = price_series.index.min()\n",
    "    # reconstitue la liste des Tk\n",
    "    Tk_list = [start + timedelta(seconds=i * (env.T // N)) for i in range(N)]\n",
    "    actions = [(Tk, Q0 / N) for Tk in Tk_list]\n",
    "    return pnl_agent(price_series, actions, Mk, a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, start_dates):\n",
    "    \"\"\"\n",
    "    start_dates : liste de datetimes à tester\n",
    "    Renvoie un dict {mean, median, glr, prob} des delta P&L vs TWAP en bp.\n",
    "    \"\"\"\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0   # mode pure greedy\n",
    "\n",
    "    deltas = []\n",
    "    for sd in start_dates:\n",
    "        # 1) repositionne l'env\n",
    "        agent.env.start_date = sd\n",
    "        state = agent.env.reset()\n",
    "\n",
    "        # 2) collect des (Tk_datetime, x_Tk)\n",
    "        actions = []\n",
    "        for Tk in agent.env.Tk_list:\n",
    "            action = agent.choose_action(state)\n",
    "            Tk_dt = sd + timedelta(seconds=int(Tk))\n",
    "            actions.append((Tk_dt, action))\n",
    "            state, _, done, _ = agent.env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # 3) calculs P&L\n",
    "        series = agent.env.price_data\n",
    "        pnl_rl  = pnl_agent(series, actions, agent.env.Mk, agent.env.a)\n",
    "        pnl_ref = pnl_twap(series, agent.env.initial_inventory,\n",
    "                          agent.env.Mk, len(agent.env.Tk_list),\n",
    "                          agent.env.a)\n",
    "\n",
    "        # 4) delta en basis points\n",
    "        deltas.append(1e4 * (pnl_rl - pnl_ref) / pnl_ref)\n",
    "\n",
    "    agent.epsilon = original_epsilon\n",
    "\n",
    "    arr = np.array(deltas)\n",
    "    stats = {\n",
    "        \"mean\"  : round(arr.mean(),3),\n",
    "        \"median\": round(np.median(arr),3),\n",
    "        \"glr\"   : round(arr[arr>0].mean() / (-arr[arr<0].mean() + 1e-8),3),\n",
    "        \"prob\"  : round(float((arr>0).mean()*100),3)\n",
    "    }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "min_time = data.index.min()\n",
    "max_time = data.index.max() - timedelta(seconds=env.T)\n",
    "# horaire plein toutes les T secondes\n",
    "test_dates = pd.date_range(min_time, max_time, freq=f\"{env.T}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 25789.547, 'median': 21119.765, 'glr': 9.425, 'prob': 87.955}\n"
     ]
    }
   ],
   "source": [
    "stats = evaluate(agent, test_dates)\n",
    "print(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
