{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "from torch.optim import RMSprop\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_time(t,T):\n",
    "    return 2*(t-T)/T+1\n",
    "\n",
    "def transformed_inventory_action(q0,q,x):\n",
    "    q_hat=q/q0-1\n",
    "    x_hat=x/q0\n",
    "    r=np.sqrt(q_hat**2+x_hat**2)\n",
    "    theta=np.arctan(-x_hat/q_hat)\n",
    "    chi=-x_hat/q_hat\n",
    "    if theta<=np.pi/4:\n",
    "        radial_dist=r*np.sqrt((chi**2+1)*(2*np.cos(np.pi/4-theta)**2))\n",
    "    else:\n",
    "        radial_dist=r*np.sqrt((chi**(-2)+1)*(2*np.cos(theta-np.pi/4)**2))\n",
    "    q_transform=-radial_dist*np.cos(theta)\n",
    "    x_transform=radial_dist*np.sin(theta)\n",
    "    return q_transform,x_transform\n",
    "\n",
    "def transformed_price(midprice_series):\n",
    "    \"\"\"\n",
    "    Computes the transformed price feature (P̃) from a time series of midprices.\n",
    "\n",
    "    Parameters:\n",
    "        midprice_series (pd.Series): Series indexed by timestamp (datetime), with midprice per second.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Transformed price series (P̃), same index as input.\n",
    "    \"\"\"\n",
    "    # Ensure the series is sorted by time\n",
    "    midprice_series = midprice_series.sort_index()\n",
    "\n",
    "    # Group by hour\n",
    "    grouped = midprice_series.groupby(midprice_series.index.floor('H'))\n",
    "\n",
    "    transformed_series = []\n",
    "\n",
    "    for hour, group in grouped:\n",
    "        # Subtract opening price of the hour\n",
    "        opening_price = group.iloc[0]\n",
    "        centered = group - opening_price\n",
    "\n",
    "        # Estimate scale to fit mostly within [-1, 1]\n",
    "        lower, upper = np.percentile(centered, [1, 99])  # clip only outliers\n",
    "        if lower!=upper:\n",
    "            # Affine transformation\n",
    "            transformed = (2/(upper-lower))*(centered-upper)+1\n",
    "        else:\n",
    "            transformed =0*centered\n",
    "        \n",
    "        transformed_series.append(transformed)\n",
    "\n",
    "    return pd.concat(transformed_series)\n",
    "\n",
    "def QV(midprice_series):\n",
    "    return np.sum(midprice_series.diff()**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv:\n",
    "    def __init__(self, start_date,T,N, delta_t,price_data, initial_inventory=500,a=1):\n",
    "        \"\"\"Time is expressed in second\"\"\"\n",
    "        self.start_date=start_date\n",
    "        self.T=T\n",
    "        self.delta_t=delta_t #period between succesive trades in min \n",
    "        self.Tk_list = np.array([T/N*i for i in range(1,N)])\n",
    "        self.Mk=T/N/delta_t\n",
    "        self.price_data = price_data\n",
    "        self.transormed_price=transformed_price(price_data)\n",
    "        self.initial_inventory = initial_inventory\n",
    "\n",
    "        self.current_period_index = 0\n",
    "        self.inventory = initial_inventory\n",
    "        self.time = self.Tk_list[0]\n",
    "        self.done = False\n",
    "        self.a=a\n",
    "\n",
    "        self.state=self.get_state(self.time)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_period_index = 0\n",
    "        self.inventory = self.initial_inventory\n",
    "        self.time = self.Tk_list[0]\n",
    "        self.done = False\n",
    "        return self.get_state(self.time)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Applique l'action (ex : quantité à vendre), met à jour l'état, retourne:\n",
    "        next_state, reward, done, info\n",
    "        \"\"\"\n",
    "        #Quantiti of shares to sell\n",
    "        x_Tk= action\n",
    "\n",
    "        #Select prices of the period following the action [T_k,T_k+1[\n",
    "        mask=(self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index])<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index+1]))\n",
    "        selected_times=self.price_data.index[mask]\n",
    "        prices=self.price_data.loc[selected_times]\n",
    "\n",
    "        #compute reward\n",
    "        reward =np.sum((self.inventory/prices.shape[0]) * prices.diff()-self.a*(x_Tk/self.Mk)**2)\n",
    "\n",
    "        self.inventory -= x_Tk\n",
    "\n",
    "        #Update current period\n",
    "        self.current_period_index += 1\n",
    "\n",
    "        #An episode ends when all the initial inventory has been sold \n",
    "        if self.inventory <= 0:\n",
    "            self.done = True\n",
    "\n",
    "        self.time = self.Tk_list[self.current_period_index]\n",
    "        next_state = self.get_state(self.time)\n",
    "\n",
    "\n",
    "        self.state=next_state\n",
    "        return next_state, reward, self.done, {}\n",
    "    \n",
    "    def get_state(self, T_i):\n",
    "\n",
    "        if self.current_period_index>0:\n",
    "            mask=(self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index-1])<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index]))\n",
    "        else:\n",
    "            mask=(self.start_date<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index]))\n",
    "            \n",
    "        selected_times=self.price_data.index[mask]\n",
    "        prices=self.price_data.loc[selected_times]\n",
    "        qv=QV(prices)\n",
    "\n",
    "        state = [\n",
    "            T_i,\n",
    "            self.inventory,\n",
    "            self.price_data.loc[self.start_date+timedelta(seconds=T_i):].values[0],\n",
    "            qv,\n",
    "        ]\n",
    "        return np.array(state, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 1)  # Q-value output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class TradingAgentRL:\n",
    "    def __init__(self, env,state_dim, epsilon=0.1, tau=0.995, gamma=0.99, batch_size=5, memory_capacity=100, update_target_freq=10, lr=1e-3):\n",
    "        self.env=env\n",
    "        self.state_dim = state_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_freq = update_target_freq\n",
    "\n",
    "        self.memory = deque(maxlen=memory_capacity)\n",
    "        self.Q_main = QNetwork(state_dim)\n",
    "        self.Q_target = QNetwork(state_dim)\n",
    "        self.Q_target.load_state_dict(self.Q_main.state_dict())\n",
    "        self.optimizer = RMSprop(self.Q_main.parameters(), lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.iteration = 0\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        q_i = state[1] \n",
    "        T_i=state[0]\n",
    "\n",
    "        ## I we have reached terminal period [TN-1,T], we sell all the inventory \n",
    "        if T_i>=self.env.Tk_list[-1]:\n",
    "            action=q_i\n",
    "        else:\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                action = np.random.binomial(q_i,1/(self.env.T-T_i))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).repeat(int(q_i)+1,1)\n",
    "                    actions_tensor = torch.arange(0, int(q_i) + 1).float()\n",
    "                    inputs = torch.cat([state_tensor, actions_tensor.unsqueeze(1)], dim=1)\n",
    "                    q_values = self.Q_main(inputs).squeeze()\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def train_step(self):\n",
    "\n",
    "        # We can update Q only if we have seen enough experience (state,action,reward,next_state)\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # We sample batch_size trasnitions from memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states = zip(*minibatch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "\n",
    "        # Target computation\n",
    "        with torch.no_grad():\n",
    "\n",
    "            next_q_values=[]\n",
    "            for j in range(self.batch_size):\n",
    "\n",
    "                q_range = int(next_states[j][1])  # inventory\n",
    "                ns_batch = next_states[j].unsqueeze(0).repeat(q_range + 1, 1)\n",
    "                actions_batch = torch.arange(0, q_range + 1).float()\n",
    "                inputs = torch.cat([ns_batch, actions_batch.unsqueeze(1)], dim=1)\n",
    "\n",
    "                if next_states[j][0]==self.env.T:\n",
    "                    next_q_value = 0\n",
    "                elif next_states[j][0]==self.env.Tk_list[-1]:\n",
    "                    #R(s,q)=q(p′ −p)−aq2,\n",
    "                    q=states[j][1]\n",
    "                    T=self.env.start_date+timedelta(seconds=self.env.T)\n",
    "                    delta=self.env.delta_t\n",
    "                    prices=self.env.price_data \n",
    "                    p=prices.loc[T:].values[0]\n",
    "                    p_prim=prices.loc[T+timedelta(seconds=delta):].values[0]\n",
    "                    next_q_value = q*(p_prim-p)-self.env.a*q**2#f(next_states[j],states[j][1])\n",
    "                else:\n",
    "                    \n",
    "\n",
    "                    #Compute Q-values with main network\n",
    "                    self.Q_main.eval()\n",
    "                    q_values = self.Q_main(inputs).squeeze()\n",
    "                    ns=torch.tensor(next_states[j])\n",
    "                    # Select action with highest Q-value\n",
    "                    action = torch.tensor([torch.argmax(q_values).item()])\n",
    "                    #Compute future Q-value with target network\n",
    "                    input_target = torch.cat([ns, action])  # shape: [5]\n",
    "                    self.Q_target.eval()\n",
    "                    next_q_value = self.Q_target(input_target.unsqueeze(0)).item()\n",
    "\n",
    "                next_q_values.append(next_q_value)        \n",
    "            \n",
    "            next_q_values = torch.FloatTensor(next_q_values)\n",
    "\n",
    "\n",
    "        targets=rewards+self.gamma*next_q_values\n",
    "        inputs = torch.cat([states ,actions.unsqueeze(1)], dim=1)\n",
    "        q_preds = self.Q_main(inputs)\n",
    "        loss = self.loss_fn(q_preds.squeeze(), targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "    def train(self, num_episodes=100, N=100, update_target_every=10, tau=0.995):\n",
    "        \"\"\"\n",
    "        Entraîne l'agent sur l'environnement `env` pendant `num_episodes` épisodes.\n",
    "        À chaque épisode, effectue N pas d'interaction.\n",
    "        \"\"\"\n",
    "        last_date=self.env.price_data.index[-1]\n",
    "        dates=self.env.price_data.loc[:last_date-timedelta(seconds=self.env.T)].index\n",
    "        for episode in range(num_episodes):\n",
    "            # One episode is defined as an execution period of lenght T, chosen randomly in the dataset (2018 to 2023)\n",
    "            self.env.start_date=random.choice(dates)\n",
    "            # State is reset at the beginning of each episode\n",
    "            state = self.env.reset()  \n",
    "            for i in range(N): # N is the number of period T0<T1..<TN-1 such that an action is taken at each T_i\n",
    "\n",
    "                #choose action according to epsilon-greedy policy\n",
    "                action =self.choose_action(state)\n",
    "\n",
    "                # Update current state of the environment \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Save transition for experience replay\n",
    "                self.store_transition(state, action, reward, next_state)\n",
    "\n",
    "                state=next_state\n",
    "\n",
    "                # Update Q with experience replay\n",
    "                self.train_step()\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Mise à jour du réseau cible\n",
    "            if episode % update_target_every == 0:\n",
    "                self.Q_target.load_state_dict(self.Q_main.state_dict())\n",
    "\n",
    "            # Décroissance de ε\n",
    "            self.epsilon = max(self.epsilon * tau, 0.01)\n",
    "\n",
    "            print(f\"Episode {episode+1}/{num_episodes} terminé, ε = {self.epsilon:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Data/BTC_ETH_15mn.csv\")\n",
    "data.Date=data.Date.apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "data=data.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=TradingEnv(data.Price.index[0],15*500*60,100,15*60,data.Price)\n",
    "agent=TradingAgentRL(env,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/2 terminé, ε = 0.0975\n",
      "Episode 2/2 terminé, ε = 0.0970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cy/p_l7v2mn57v5jtdrn3qvp_0h0000gq/T/ipykernel_79837/4259920681.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ns=torch.tensor(next_states[j])\n"
     ]
    }
   ],
   "source": [
    "agent.train( num_episodes=2, N=70, update_target_every=10, tau=0.995)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
