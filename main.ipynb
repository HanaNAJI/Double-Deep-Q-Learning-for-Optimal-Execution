{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yasser/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "from torch.optim import RMSprop\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_time(t,T):\n",
    "    return 2*(t-T)/T+1\n",
    "\n",
    "def transformed_inventory_action(q0,q,x):\n",
    "    q_hat=q/q0-1\n",
    "    x_hat=x/q0\n",
    "    r=np.sqrt(q_hat**2+x_hat**2)\n",
    "    theta=np.arctan(-x_hat/q_hat)\n",
    "    chi=-x_hat/q_hat\n",
    "    if theta<=np.pi/4:\n",
    "        radial_dist=r*np.sqrt((chi**2+1)*(2*np.cos(np.pi/4-theta)**2))\n",
    "    else:\n",
    "        radial_dist=r*np.sqrt((chi**(-2)+1)*(2*np.cos(theta-np.pi/4)**2))\n",
    "    q_transform=-radial_dist*np.cos(theta)\n",
    "    x_transform=radial_dist*np.sin(theta)\n",
    "    return q_transform,x_transform\n",
    "\n",
    "def transformed_price(midprice_series):\n",
    "    \"\"\"\n",
    "    Computes the transformed price feature (P̃) from a time series of midprices.\n",
    "\n",
    "    Parameters:\n",
    "        midprice_series (pd.Series): Series indexed by timestamp (datetime), with midprice per second.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Transformed price series (P̃), same index as input.\n",
    "    \"\"\"\n",
    "    # Ensure the series is sorted by time\n",
    "    midprice_series = midprice_series.sort_index()\n",
    "\n",
    "    # Group by hour\n",
    "    grouped = midprice_series.groupby(midprice_series.index.floor('H'))\n",
    "\n",
    "    transformed_series = []\n",
    "\n",
    "    for hour, group in grouped:\n",
    "        # Subtract opening price of the hour\n",
    "        opening_price = group.iloc[0]\n",
    "        centered = group - opening_price\n",
    "\n",
    "        # Estimate scale to fit mostly within [-1, 1]\n",
    "        lower, upper = np.percentile(centered, [1, 99])  # clip only outliers\n",
    "        if lower!=upper:\n",
    "            # Affine transformation\n",
    "            transformed = (2/(upper-lower))*(centered-upper)+1\n",
    "        else:\n",
    "            transformed =0*centered\n",
    "        \n",
    "        transformed_series.append(transformed)\n",
    "\n",
    "    return pd.concat(transformed_series)\n",
    "\n",
    "def QV(midprice_series):\n",
    "    return np.sum(midprice_series.diff()**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv:\n",
    "    def __init__(self, start_date,T,N, delta_t,price_data, initial_inventory=500,a=1):\n",
    "        \"\"\"Time is expressed in second\"\"\"\n",
    "        self.start_date=start_date\n",
    "        self.T=T\n",
    "        self.delta_t=delta_t #period between succesive trades in min \n",
    "        self.Tk_list = np.array([T/N*i for i in range(1,N)])\n",
    "        self.Mk=T/N/delta_t\n",
    "        self.price_data = price_data\n",
    "        self.transormed_price=transformed_price(price_data)\n",
    "        self.initial_inventory = initial_inventory\n",
    "\n",
    "        self.current_period_index = 0\n",
    "        self.inventory = initial_inventory\n",
    "        self.time = self.Tk_list[0]\n",
    "        self.done = False\n",
    "        self.a=a\n",
    "\n",
    "        self.state=self.get_state(self.time)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_period_index = 0\n",
    "        self.inventory = self.initial_inventory\n",
    "        self.time = self.Tk_list[0]\n",
    "        self.done = False\n",
    "        return self.get_state(self.time)\n",
    "\n",
    "    def step(self, action):\n",
    "            x_Tk = action\n",
    "            # bornes de temps de la période courante\n",
    "            t0 = self.start_date + timedelta(seconds=self.Tk_list[self.current_period_index])\n",
    "            t1 = self.start_date + timedelta(seconds=self.Tk_list[self.current_period_index + 1])\n",
    "            prices = self.price_data.loc[t0:t1]\n",
    "            n = len(prices)\n",
    "\n",
    "            # 1) reward de trade intra-période\n",
    "            if n > 0:\n",
    "                trade_reward = np.sum((self.inventory / n) * prices.diff())\n",
    "            else:\n",
    "                trade_reward = 0.0\n",
    "\n",
    "            # 2) pénalité quadratique (on ne divise par Mk que si Mk>0)\n",
    "            if self.Mk > 0:\n",
    "                penalty = self.a * (x_Tk / self.Mk) ** 2\n",
    "            else:\n",
    "                penalty = 0.0\n",
    "\n",
    "            reward = trade_reward - penalty\n",
    "\n",
    "            # mise à jour de l'état\n",
    "            self.inventory -= x_Tk\n",
    "            self.current_period_index += 1\n",
    "            if self.inventory <= 0 or self.current_period_index >= len(self.Tk_list) - 1:\n",
    "                self.done = True\n",
    "\n",
    "            next_t = self.Tk_list[min(self.current_period_index, len(self.Tk_list)-1)]\n",
    "            next_state = self.get_state(next_t)\n",
    "            return next_state, reward, self.done, {}\n",
    "        \n",
    "    def get_state(self, T_i):\n",
    "\n",
    "        if self.current_period_index>0:\n",
    "            mask=(self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index-1])<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index]))\n",
    "        else:\n",
    "            mask=(self.start_date<=self.price_data.index)&(self.price_data.index<self.start_date+timedelta(seconds=self.Tk_list[self.current_period_index]))\n",
    "            \n",
    "        selected_times=self.price_data.index[mask]\n",
    "        prices=self.price_data.loc[selected_times]\n",
    "        qv=QV(prices)\n",
    "\n",
    "        state = [\n",
    "            T_i,\n",
    "            self.inventory,\n",
    "            self.price_data.loc[self.start_date+timedelta(seconds=T_i):].values[0],\n",
    "            qv,\n",
    "        ]\n",
    "        return np.array(state, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, 1)  # Q-value output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class TradingAgentRL:\n",
    "    def __init__(self, env,state_dim, epsilon=0.1, tau=0.995, gamma=0.99, batch_size=5, memory_capacity=100, update_target_freq=10, lr=1e-3):\n",
    "        self.env=env\n",
    "        self.state_dim = state_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_freq = update_target_freq\n",
    "\n",
    "        self.memory = deque(maxlen=memory_capacity)\n",
    "        self.Q_main = QNetwork(state_dim)\n",
    "        self.Q_target = QNetwork(state_dim)\n",
    "        self.Q_target.load_state_dict(self.Q_main.state_dict())\n",
    "        self.optimizer = RMSprop(self.Q_main.parameters(), lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.iteration = 0\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        q_i = state[1] \n",
    "        T_i=state[0]\n",
    "\n",
    "        ## I we have reached terminal period [TN-1,T], we sell all the inventory \n",
    "        if T_i>=self.env.Tk_list[-1]:\n",
    "            action=q_i\n",
    "        else:\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                action = np.random.binomial(q_i,1/(self.env.T-T_i))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).repeat(int(q_i)+1,1)\n",
    "                    actions_tensor = torch.arange(0, int(q_i) + 1).float()\n",
    "                    inputs = torch.cat([state_tensor, actions_tensor.unsqueeze(1)], dim=1)\n",
    "                    q_values = self.Q_main(inputs).squeeze()\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def train_step(self):\n",
    "\n",
    "        # We can update Q only if we have seen enough experience (state,action,reward,next_state)\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # We sample batch_size trasnitions from memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states = zip(*minibatch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "\n",
    "        # Target computation\n",
    "        with torch.no_grad():\n",
    "\n",
    "            next_q_values=[]\n",
    "            for j in range(self.batch_size):\n",
    "\n",
    "                q_range = int(next_states[j][1])  # inventory\n",
    "                ns_batch = next_states[j].unsqueeze(0).repeat(q_range + 1, 1)\n",
    "                actions_batch = torch.arange(0, q_range + 1).float()\n",
    "                inputs = torch.cat([ns_batch, actions_batch.unsqueeze(1)], dim=1)\n",
    "\n",
    "                if next_states[j][0]==self.env.T:\n",
    "                    next_q_value = 0\n",
    "                elif next_states[j][0]==self.env.Tk_list[-1]:\n",
    "                    #R(s,q)=q(p′ −p)−aq2,\n",
    "                    q=states[j][1]\n",
    "                    T=self.env.start_date+timedelta(seconds=self.env.T)\n",
    "                    delta=self.env.delta_t\n",
    "                    prices=self.env.price_data \n",
    "                    p=prices.loc[T:].values[0]\n",
    "                    p_prim=prices.loc[T+timedelta(seconds=delta):].values[0]\n",
    "                    next_q_value = q*(p_prim-p)-self.env.a*q**2#f(next_states[j],states[j][1])\n",
    "                else:\n",
    "                    \n",
    "\n",
    "                    #Compute Q-values with main network\n",
    "                    self.Q_main.eval()\n",
    "                    q_values = self.Q_main(inputs).squeeze()\n",
    "                    ns=torch.tensor(next_states[j])\n",
    "                    # Select action with highest Q-value\n",
    "                    action = torch.tensor([torch.argmax(q_values).item()])\n",
    "                    #Compute future Q-value with target network\n",
    "                    input_target = torch.cat([ns, action])  # shape: [5]\n",
    "                    self.Q_target.eval()\n",
    "                    next_q_value = self.Q_target(input_target.unsqueeze(0)).item()\n",
    "\n",
    "                next_q_values.append(next_q_value)        \n",
    "            \n",
    "            next_q_values = torch.FloatTensor(next_q_values)\n",
    "\n",
    "\n",
    "        targets=rewards+self.gamma*next_q_values\n",
    "        inputs = torch.cat([states ,actions.unsqueeze(1)], dim=1)\n",
    "        q_preds = self.Q_main(inputs)\n",
    "        loss = self.loss_fn(q_preds.squeeze(), targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "    def train(self, num_episodes=100, N=100, update_target_every=10, tau=0.995):\n",
    "        \"\"\"\n",
    "        Entraîne l'agent sur l'environnement `env` pendant `num_episodes` épisodes.\n",
    "        À chaque épisode, effectue N pas d'interaction.\n",
    "        \"\"\"\n",
    "        last_date=self.env.price_data.index[-1]\n",
    "        dates=self.env.price_data.loc[:last_date-timedelta(seconds=self.env.T)].index\n",
    "        for episode in range(num_episodes):\n",
    "            # One episode is defined as an execution period of lenght T, chosen randomly in the dataset (2018 to 2023)\n",
    "            self.env.start_date=random.choice(dates)\n",
    "            # State is reset at the beginning of each episode\n",
    "            state = self.env.reset()  \n",
    "            for i in range(N): # N is the number of period T0<T1..<TN-1 such that an action is taken at each T_i\n",
    "\n",
    "                #choose action according to epsilon-greedy policy\n",
    "                action =self.choose_action(state)\n",
    "\n",
    "                # Update current state of the environment \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Save transition for experience replay\n",
    "                self.store_transition(state, action, reward, next_state)\n",
    "\n",
    "                state=next_state\n",
    "\n",
    "                # Update Q with experience replay\n",
    "                self.train_step()\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Mise à jour du réseau cible\n",
    "            if episode % update_target_every == 0:\n",
    "                self.Q_target.load_state_dict(self.Q_main.state_dict())\n",
    "\n",
    "            # Décroissance de ε\n",
    "            self.epsilon = max(self.epsilon * tau, 0.01)\n",
    "\n",
    "            print(f\"Episode {episode+1}/{num_episodes} terminé, ε = {self.epsilon:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Data/BTC_ETH_15mn.csv\")\n",
    "data.Date=data.Date.apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "data=data.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Price (ETH)</th>\n",
       "      <th>Volume (ETH)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-10-24 00:00:00</th>\n",
       "      <td>6476.540000</td>\n",
       "      <td>3.708780e+09</td>\n",
       "      <td>204.327000</td>\n",
       "      <td>1232950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24 00:15:00</th>\n",
       "      <td>6483.150000</td>\n",
       "      <td>3.709670e+09</td>\n",
       "      <td>204.378000</td>\n",
       "      <td>1236570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24 00:30:00</th>\n",
       "      <td>6474.200000</td>\n",
       "      <td>3.677130e+09</td>\n",
       "      <td>204.439000</td>\n",
       "      <td>1239840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24 00:45:00</th>\n",
       "      <td>6486.190000</td>\n",
       "      <td>3.661460e+09</td>\n",
       "      <td>204.441000</td>\n",
       "      <td>1237220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24 01:00:00</th>\n",
       "      <td>6480.084479</td>\n",
       "      <td>3.641914e+09</td>\n",
       "      <td>204.070635</td>\n",
       "      <td>1227899090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-27 08:45:00</th>\n",
       "      <td>37346.223298</td>\n",
       "      <td>1.628826e+10</td>\n",
       "      <td>2046.790071</td>\n",
       "      <td>9322101316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-27 09:00:00</th>\n",
       "      <td>37401.430405</td>\n",
       "      <td>1.635176e+10</td>\n",
       "      <td>2049.064393</td>\n",
       "      <td>9328586333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-27 09:15:00</th>\n",
       "      <td>37405.996546</td>\n",
       "      <td>1.648487e+10</td>\n",
       "      <td>2049.523629</td>\n",
       "      <td>9353842431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-27 09:30:00</th>\n",
       "      <td>37432.627567</td>\n",
       "      <td>1.656480e+10</td>\n",
       "      <td>2051.479080</td>\n",
       "      <td>9362712712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-27 09:45:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9366213874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178444 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Price        Volume  Price (ETH)  Volume (ETH)\n",
       "Date                                                                      \n",
       "2018-10-24 00:00:00   6476.540000  3.708780e+09   204.327000    1232950000\n",
       "2018-10-24 00:15:00   6483.150000  3.709670e+09   204.378000    1236570000\n",
       "2018-10-24 00:30:00   6474.200000  3.677130e+09   204.439000    1239840000\n",
       "2018-10-24 00:45:00   6486.190000  3.661460e+09   204.441000    1237220000\n",
       "2018-10-24 01:00:00   6480.084479  3.641914e+09   204.070635    1227899090\n",
       "...                           ...           ...          ...           ...\n",
       "2023-11-27 08:45:00  37346.223298  1.628826e+10  2046.790071    9322101316\n",
       "2023-11-27 09:00:00  37401.430405  1.635176e+10  2049.064393    9328586333\n",
       "2023-11-27 09:15:00  37405.996546  1.648487e+10  2049.523629    9353842431\n",
       "2023-11-27 09:30:00  37432.627567  1.656480e+10  2051.479080    9362712712\n",
       "2023-11-27 09:45:00           NaN           NaN          NaN    9366213874\n",
       "\n",
       "[178444 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=TradingEnv(data.Price.index[0],15*500*60,100,15*60,data.Price)\n",
    "agent=TradingAgentRL(env,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-e62f627eef0b>:85: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  states = torch.FloatTensor(states)\n",
      "<ipython-input-3-e62f627eef0b>:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ns=torch.tensor(next_states[j])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/2 terminé, ε = 0.0995\n",
      "Episode 2/2 terminé, ε = 0.0990\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent.train( num_episodes=2, N=70, update_target_every=10, tau=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnl_agent(price_series, start_date, actions, delta_t, a, initial_inventory):\n",
    "    \"\"\"\n",
    "    Eq (3.1)-(3.2) :\n",
    "      R_{k,i} = q_{t_{k,i}}*(p_{i+1}-p_i) - a*(x_k/M_k)^2\n",
    "      P&L = sum_{k,i} R_{k,i}  (avec q_{t_{k,0}} = q0 au début de chaque k)\n",
    "    \"\"\"\n",
    "    pnl = 0.0\n",
    "    q = initial_inventory\n",
    "    p0 = price_series.loc[start_date]\n",
    "    pnl -= q * p0  # terme -q0*p0 parfois inclus dans la forme développée (3.4)\n",
    "\n",
    "    for Tk, xk in actions:\n",
    "        # récupérer les prix de Tk à Tk+1\n",
    "        block = price_series.loc[Tk : Tk + timedelta(seconds=delta_t)]\n",
    "        prices = block.values\n",
    "        M = len(prices) - 1\n",
    "        if M <= 0:\n",
    "            continue\n",
    "\n",
    "        v = xk / M  # quantité par sub-tick\n",
    "        for i in range(M):\n",
    "            dp = prices[i+1] - prices[i]\n",
    "            pnl += q * dp            # q_{t_{k,i}} * (p_{i+1}-p_i)\n",
    "            pnl -= a * (v ** 2)      # pénalité quadratique\n",
    "            q  -= v                  # on diminue q au fil des sous-ticks\n",
    "\n",
    "    return pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnl_twap(price_series, start_date, Q0, Tk_list, delta_t, a):\n",
    "    \"\"\"\n",
    "    Eq (7.1): x_k = Q0/N\n",
    "    Puis on appelle pnl_agent avec ce plan d'actions.\n",
    "    \"\"\"\n",
    "    N = len(Tk_list) - 1\n",
    "    x_per = Q0 / N\n",
    "    actions = [\n",
    "        (start_date + timedelta(seconds=int(Tk_list[k])), x_per)\n",
    "        for k in range(N)\n",
    "    ]\n",
    "    return pnl_agent(price_series, start_date, actions, delta_t, a, Q0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "min_time = data.index.min()\n",
    "max_time = data.index.max() - timedelta(seconds=env.T)\n",
    "# horaire plein toutes les T secondes\n",
    "test_dates = pd.date_range(min_time, max_time, freq=f\"{env.T}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, start_dates):\n",
    "    \"\"\"\n",
    "    Pure greedy evaluation (ε=0) : compare P&L_RL vs P&L_TWAP en bps.\n",
    "    Renvoie mean, median, glr, prob.\n",
    "    \"\"\"\n",
    "    orig_eps = agent.epsilon\n",
    "    agent.epsilon = 0.0\n",
    "\n",
    "    deltas = []\n",
    "    for sd in start_dates:\n",
    "        # tronquer la série à [sd, sd+T]\n",
    "        series = agent.env.price_data.loc[\n",
    "            sd : sd + timedelta(seconds=agent.env.T)\n",
    "        ]\n",
    "\n",
    "        # reset env\n",
    "        agent.env.start_date = sd\n",
    "        state = agent.env.reset()\n",
    "\n",
    "        # collecter actions greedy\n",
    "        actions = []\n",
    "        for Tk in agent.env.Tk_list[:-1]:\n",
    "            x = agent.choose_action(state)\n",
    "            t_dt = sd + timedelta(seconds=int(Tk))\n",
    "            actions.append((t_dt, x))\n",
    "            state, _, done, _ = agent.env.step(x)\n",
    "            if done: break\n",
    "\n",
    "        # calcul des P&L\n",
    "        pnl_rl  = pnl_agent(\n",
    "            series, sd, actions,\n",
    "            agent.env.delta_t,\n",
    "            agent.env.a,\n",
    "            agent.env.initial_inventory\n",
    "        )\n",
    "        pnl_ref = pnl_twap(\n",
    "            series, sd,\n",
    "            agent.env.initial_inventory,\n",
    "            agent.env.Tk_list,\n",
    "            agent.env.delta_t,\n",
    "            agent.env.a\n",
    "        )\n",
    "\n",
    "        if pnl_ref != 0:\n",
    "            deltas.append(1e4 * (pnl_rl - pnl_ref) / pnl_ref)\n",
    "\n",
    "    agent.epsilon = orig_eps\n",
    "\n",
    "    arr = np.array(deltas, dtype=float)\n",
    "    return {\n",
    "        \"mean\"  : round(arr.mean(),    3),\n",
    "        \"median\": round(np.median(arr),3),\n",
    "        \"glr\"   : round(arr[arr>0].mean() / (-arr[arr<0].mean() + 1e-8), 3),\n",
    "        \"prob\"  : round(float((arr>0).mean() * 100), 3),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 393.279, 'median': 295.783, 'glr': 3.848, 'prob': 88.515}\n"
     ]
    }
   ],
   "source": [
    "stats = evaluate(agent, test_dates)\n",
    "print(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
